%#####################################################################################################################
% Datei	: Conclusion.tex
% Autor	: Byron Worms
%#####################################################################################################################
%---------------------------------------------------------------------------------------------------------------------
% Future work
%---------------------------------------------------------------------------------------------------------------------
\section{Abschluss}
\label{sec:conclusion}
In dem vorliegenden technischen Paper wurden vier verschiedene Algorithmen aus dem Bereich \textit{Perceptual Hashing}
über den Bildraum mit Hilfe mehrerer verschieden definierten Experimenten analysiert und ausgewertet. Die Ergebnisse
der Untersuchungen werden nachfolgend zusammengefasst aufgeführt.
\\[1em]
Jeder eingesetzte Algorithmus arbeitet auf Basis des strukturellen Aufbaus des Bildmaterials (eingeteilt in hochfrequente
und niederfrequente Anteile). Die Komplexität der Struktur hat dabei einen erheblichen Einfluss auf die Zuordnungsqualität
während der Bestimmung von Ähnlichkeitsmaßen. Die Bedeutsamkeit der Bildmodifikationen steigt parallel mit der Erhöhung
der strukturellen Transparenz des Bildmaterials, sodass bereits leichte Veränderungen bei einfachen Bildinhalten zu großen
Änderungen im Aufbau führen.
\\
Besonders der \textit{Wavelet}--Algorithmus ist durch die Adressierung von hochfrequenten Bildbereichen (vgl. grundlegende
Funktionsweisen von grafischen Kantendetektierungen) anfällig gegenüber geringfügigen Veränderungen. Dahingegen sind
Rotationsänderungen (zum Beispiel horizontales Spiegel) eine Schwachstelle, die alle der vier verwendeten Algorithmen betrifft.
\\
Die Auswertung des durchgeführten Quervergleichs der Bildkategorie \textit{Komplexe Bilder} führte zu dem Ergebnis,
dass die \textit{RADISH}--Vorgehensweise durch ähnliche Intensitätsverteilungen in den abgeglichenen Bildmaterial zu einer
falschen Zuordnung von ähnlichen Bildpaaren neigt. Die Erkennungsqualitäten des \textit{BMB}--Verfahrens sinken dahingegen
mit der gleichzeitigen Verringerungen des Thresholds.
\\[1em]
Ungeachtet der gemessenen Erkennungsfehlerraten operieren die Algorithmen \textit{BMB} und \textit{RADISH} durch die
ausschließliche Nutzung einfacher Pixeloperationen effizienter als die Verfahren \textit{DCT} sowie \textit{Wavelet},
die kostenintensive Operationen auf Pixelebene einsetzen.
\\
In einem direkten Vergleich der vier Algorithmen zu der nativen Herangehensweise mittels \textit{Kreuzkorrelation} wurde
eine durchschnittliche Geschwindigkeitssteigerung von bis zu \hbox{$\sim10500\%$} protokolliert.
\\[1em]
Auf Basis der gemessenen Erkennungsraten sowie Berechnungszeiten besteht die Möglichkeit, ein \textit{Ranking} der vier
Algorithmen zu Gunsten der nachstehenden Bewertungsgruppen zu definieren: \textit{FRR}, \textit{FAR}, \textit{Berechnungszeiten}
und die Gesamtwertung.
\\
Die einzelnen Fehlerraten und Zeiten werden dabei aufaddiert und anschließend durch die kumulierte obere Grenze dividiert.
Die vereinfachte Rechnung ist anwendbar, da alle quantifizierten Messwerte in dem Bereich von \hbox{$[0,1]$} liegen. Trotzdem
ist die nachstehende \textit{Ranking}--Tabelle~\ref{tab:conclusion_ranking} nur ein Schätzmaß für die allgemeine Qualität und
Effizienz der Algorithmen. Zum einem werden nicht vereinbare Einheiten miteinander in Verbindung gesetzt (Fehlerraten und Sekunden)
und zum anderen sind die in Abschnitt~\ref{sec:experimental} ermittelten Maße abhängig von dem ausgewählten Bildmaterial.
Ein Austausch des eingesetzten Materials kann dementsprechend zu einer differenzierenden \textit{Ranking}--Tabelle führen.
\begin{table}[H]
	\begin{center}
		\begin{tabular}{|l|c|c|c|c|}
			\mytoprule
			\centering\bfseries Eigenschaft & \bfseries \textit{RADISH} & \bfseries \textit{DCT} & \bfseries \textit{Wavelet} & \bfseries \textit{BMB}
			\\
			\hline
			\hline
			%---------------------------------------------------------------------------------------------------------------------	
			\textit{FRR} & 0,39 & 0,469 & \cellcolor{myorange}0,828 & \cellcolor{mygreen}0,256
			\\
			\hline
			%---------------------------------------------------------------------------------------------------------------------	
			\textit{FAR} & \cellcolor{myorange}0,028 & 0,0009 & \cellcolor{mygreen}0 & 0,02
			\\
			\hline	
			%---------------------------------------------------------------------------------------------------------------------	
			\textit{Geschwindigkeit} & 0,107s & 0,223s & \cellcolor{myorange}0,362s & \cellcolor{mygreen}0,038s 
			\\	
			\hline
			\hline
			%---------------------------------------------------------------------------------------------------------------------	
			\textit{Gesamt} & 0,175 & 0,23 & \cellcolor{myorange}0,39 & \cellcolor{mygreen}0,104 
			\\																						
			%---------------------------------------------------------------------------------------------------------------------
			\mybottomrule			
		\end{tabular}
		\caption{Ranking der Algorithmen (Eigene Darstellung)}
		\label{tab:conclusion_ranking}
	\end{center}
\end{table}
\noindent
Der Algorithmus mit den besten Werten bezüglich der betrachteten Eigenschaft wird mit der Farbe {\color{mygreen}\rule[0cm]{0.2cm}{0.2cm}}
hervorgehoben, wohingegen das Verfahren mit den schlechtesten Werten mit der Farbe {\color{myorange}\rule[0cm]{0.2cm}{0.2cm}} kodiert
wird.

\subsection{Zusammenfassung}
\label{sec:summary}
In den vorangegangenen Abschnitten wurde ein alternativer Ansatz, \textit{Perceptual Hashing}, für die Authentifizierungs--
und Integritätsprüfung von Bildmaterial eingehend analysiert und bewertet. Die vier untersuchten Algorithmen (\textit{DCT},
\textit{RADISH}, \textit{Wavelet} und \textit{BMB}) adressierten dabei
unterschiedliche Herangehensweisen bezüglich der Extraktion von fundamentalen und bildabhängigen Merkmalen sowie die daraus
abgeleitete Berechnung eines gemeinsamen Ähnlichkeitsmaßes. Die Ansteuerung der in \textit{pHash} implementierten Algorithmen
wurde mittels der eigen konzipierten und umgesetzten Benutzerschnittstelle (vgl. Anhang~\ref{sec:appendix_b}) realisiert.
\\
Für die potentielle Detektierung einer möglichen Korrelation zwischen den bildlichen Strukturen und den internen Funktionsweisen
der ausgewählten Verfahren, erfolgte eine Zusammenstellung von variierenden, in Gruppen eingeteilten Bildmaterials, das ähnliche
sowie nicht zueinander ähnliche Bilder enthielt (vgl. Abschnitt~\ref{sec:solution_testdata}).
\\[1em]
Die Analyse und Protokollierung der Zuordnungs-- sowie Fehlerraten für die einzelnen Algorithmen erfolgte im Abschnitt~\ref{sec:experimental}.
Die definierten und durchgeführten Experimente dienten einer detaillierten Untersuchung der verschiedenen algorithmischen
Vorgehensweisen hinsichtlich des differenzierenden Bildmaterials. Potentielle Effizienzsteigerungen, durch den Einsatz von
Verfahren aus dem Bereich \textit{Perceptual Hashing} gegenüber dem nativen Ansatz der \textit{Kreuzkorrelation} wurden darüber hinaus
experimentell dargelegt.
\\[1em]
Die theoretische und technische Auswertung der resultierten Ergebnissen sowie Beobachtungen wird in Abschnitt~\ref{sec:results} behandelt.
Neben dem Aufzeigen von fundamentalen Voraussetzungen werden zusätzlich vertiefte Erläuterungen zwischen den gemessenen Maßeinheiten und
der internen Arbeitsweise der andersartigen Algorithmen geschildert.  
\\
Der Erkenntnisgewinn sowohl aus den Experimenten als auch aus der eingehenden Analyse der aufgezeichneten Maßeinheiten werden in
Abschnitt~\ref{sec:conclusion} (\textit{Abschluss}) kurz und bündig zusammengefasst. Es erfolgt ebenfalls eine Definition für ein
mögliches \textit{Ranking} der eingesetzten Algorithmen.

\subsection{Ausblick}
\label{subsec:futureWork}
Der Testumfang in den durchgeführten Versuchen adressiert lediglich vier unterschiedliche Ansätze für die Kategorie \textit{Perceptual Hashing}.
Eine mögliche Erweiterung besteht in der Berücksichtigung weiterer Vorgehensweisen für die Berechnung von Perceptual Hashwerten und
deren anschließender Vergleich. Beispiele für Kandidaten sind unter anderem der \textit{Average Hash}~\cite{AHASH} oder \textit{Histogramm basierte}
Ansätze~\cite{HHASH}.
\\[1em]
Die konzipierte Benutzerschnittstelle bietet einen bereits großen Umfang für die Berechnung von unterschiedlichen Hashwerten und
die anschließende Bestimmung von Ähnlichkeitsmaßen. Elementare Zeitgrößen werden dabei parallel berechnet. Potentielle Verbesserungen
können beispielsweise durch die Integration von weiteren Algorithmen, der Implementierung von Optimierungen für die Handhabung unterschiedlichen
Bildmaterials sowie die Ansteuerung von netzwerkbasierten Datenquellen realisiert werden.